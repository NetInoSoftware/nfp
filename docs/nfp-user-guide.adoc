= NetinoFastPath User Guide
:toc:

== Introduction

<<NetinoFastPath>> is a high performance user space TCP/IP stack built on top
of <<OpenDataPlane>> APIs. <<NetinoFastPath>> project is a spin-off of the
<<OpenFastPath>> open source project with which shares concepts and designs.

The purpose of this
document is to provide basic guidelines for developers of NetinoFastPath (NFP)
applications. Note that NFP repository on <<GitHub>> contains a number of
example applications in the "example" folder to check in parallel with reading
this document.

== Overview of NFP features and supported protocols

NetinoFastPath functionality is provided as a library to the applications that use
OpenDataPlane (ODP) "run to completion" execution model and framework.

Currently NFP can be used on top of:

- linux-generic implementation of ODP
- DPDK-based platforms (ODP DPDK)

Support for other operating systems (like BSD) is planned in the future.

NFP includes the following main features:

- TCP and UDP termination
- ICMP
- ARP/NDP
- IPv4 and IPv6 forwarding and routing
- IPv4 fragmentation and reassembly
- IPsec
- VRF for IPv4
- IGMP and multicast
- VLAN
- VXLAN and GRE tunneling
- DHCP IPv4 client
- DNS

Integration with OS networking stack (slowpath) is done through the TAP
interfaces. Unsupported functionality is provided by the slowpath.

NFP command line interface (CLI) provides following functions:

- Packet capture and setting debug levels
- Showing statistics, ARP table, routes and interfaces
- Configuration of routes and interfaces with VRF support

See NFP technical <<overview>> for more details about NFP design and features.

== Quick start guide

This chapter provides step-by-step instructions for installing NFP and running
an example application. The instructions were verified with Ubuntu 20.04 LTS
distribution of Linux OS; other distributions may require slightly different
tools and commands.

=== Installing ODP/NFP

Ensure first that needed SW and testing tools are installed in your environment:

The following packages (+ dependecies) are mandatory for building the
NFP applications:

    git autotools-dev libtool automake build-essential pkg-config

The following packages are optional:

    libssl-dev doxygen asciidoc valgrind libcunit1-dev libconfig-dev

Additionally, the following packages may be needed (e.g. with DPDK):

    libnuma-dev libpcap-dev python

ODP can be eiter built manually from the sources or built with devbuild script.

Download and build ODP (manually):

 git clone https://github.com/OpenDataPlane/odp-dpdk
 cd odp-dpdk
 git checkout v1.37.1.0_DPDK_19.11
 ./bootstrap
 ./configure --prefix=<INSTALL ODP TO THIS DIR> --enable-dpdk
   --enable-dpdk-zero-copy --with-platform=linux-generic
 make install

If DPDK was not installed in the default location, set the PKG_CONFIG_PATH:

 PKG_CONFIG_PATH=<DPDK INSTALL DIR>/lib64/pkgconfig ./configure
   --prefix=<INSTALL ODP TO THIS DIR> --enable-dpdk
   --enable-dpdk-zero-copy --with-platform=linux-generic

Download and build NFP applications (manually):

 git clone https://github.com/NetInoSoftware/nfp
 cd nfp
 ./bootstrap
 ./configure --prefix=<INSTALL NFP TO THIS DIR> --with-odp=<ODP INSTALLATION DIR>
 make install

Check output of `./configure --help` command for available configuration
options.

Download and build NFP applications, odp-dpdk and dpdk (devbuild script):

 git clone https://github.com/NetInoSoftware/nfp
 cd nfp
 ./scripts/devbuild_all.sh

'scripts/devbuild_all/install' is the install folder of the build.

=== The Configuration File

Many NFP initialization parameters may be set using a configuration
file. This feature utilizes the <<libconfig>> library, which is LGPL
licensed.

See config/README file for a complete list of available parameters.

See example configuration file (*.conf) from 'config' folder.

The configuration file is enabled by setting NFP_CONF_FILE environment
variable to point the configuration file.

export NFP_CONF_FILE=<full path>/nfp/config/nfp_flv_default.conf

Else, a default configuration file is checked and applied if found:
$(sysconfdir)/nfp.conf, normally /usr/local/etc/nfp.conf

=== Starting an example application

This chapter will guide you through the steps needed for starting one of the
example NFP applications. Simple webserver (located at example/webserver/
directory) is used as an example.

Choose which interface(s) in your system will be assigned for fastpath
processing, e.g. ens5f1:

 [nfp]# ip a
 ...
 21: ens5f1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP
    link/ether 80:61:5f:0c:d5:f5 brd ff:ff:ff:ff:ff:ff
	.....
 ...

Check how many processor cores are available:

 nproc

Set IP address for the interface into ./example/webserver/nfp.cli
file:

 [nfp]# cat ./example/webserver/nfp.cli
 loglevel set error
 ifconfig fp0 192.168.100.10/24

Select a web root directory. Default value is '/var/www'.

Check usage and available options with command:

 ./example/webserver/nfp_webserver --help

Start the application with command like:

 ./example/webserver/nfp_webserver -i ens5f1 -c 4 -f ./example/webserver/nfp.cli -r /var/www

Here, the number of requested fastpath (worker) processing cores is 4. However
not all the cores can be used for fastpath processing: first two are usually
restricted. Application will start 2 working threads for processing incoming
packets and 1 control thread.
Below is an example of startup output:

----
[nfp_ent]# example/webserver/nfp_webserver -i ens5f1 -c 4 -f ./example/webserver/nfp.cli -r /var/www
RLIMIT_CORE: 0/-1
Setting to max: 0
HW time counter freq: 2494224682 hz

System config:
  system.thread_count_max: 256

Pool config:
  pool.local_cache_size: 256
  pool.burst_size: 32
  pool.pkt.max_num: 262143
  pool.pkt.max_len: 65536
  pool.pkt.base_align: 64
  pool.buf.min_align: 64

Queue config:
  queue_basic.max_queue_size: 8192
  queue_basic.default_queue_size: 4096

Using scheduler 'basic'
Scheduler config:
  sched_basic.prio_spread: 4
  sched_basic.prio_spread_weight: 63
  sched_basic.load_balance: 1
  sched_basic.burst_size_default[] =  32  32  32  32  32  16   8   4
  sched_basic.burst_size_max[] =     255 255 255 255 255  16  16   8
  sched_basic.group_enable.all: 1
  sched_basic.group_enable.worker: 1
  sched_basic.group_enable.control: 1
  dynamic load balance: ON

Packet IO config:
  pktio.pktin_frame_offset: 0

PKTIO: initialized loop interface.
PKTIO: initialized dpdk pktio, use export ODP_PKTIO_DISABLE_DPDK=1 to disable.
PKTIO: initialized pcap interface.
PKTIO: initialized null interface.
PKTIO: initialized socket mmap, use export ODP_PKTIO_DISABLE_SOCKET_MMAP=1 to disable.
PKTIO: initialized socket mmsg, use export ODP_PKTIO_DISABLE_SOCKET_MMSG=1 to disable.
EAL: Detected 12 lcore(s)
EAL: Detected 1 NUMA nodes
EAL: Auto-detected process type: PRIMARY
EAL: Detected static linkage of DPDK
EAL: Multi-process socket /var/run/dpdk/145339_/mp_socket
EAL: Selected IOVA mode 'PA'
EAL: No available hugepages reported in hugepages-1048576kB
EAL: Probing VFIO support...
EAL: VFIO support initialized
EAL: No legacy callbacks, legacy socket not created

DPDK version: DPDK 20.11.4-rc1
I 20 9:3431398400 nfp_uma.c:43] Creating pool 'udp_inpcb', nitems=1024 size=1160 total=1187840
I 21 9:3431398400 nfp_uma.c:43] Creating pool 'udp_inphd', nitems=1024 size=40 total=40960
I 21 9:3431398400 nfp_uma.c:43] Creating pool 'tcp_inpcb', nitems=1024 size=1160 total=1187840
I 21 9:3431398400 nfp_uma.c:43] Creating pool 'tcp_inphd', nitems=1024 size=40 total=40960
I 21 9:3431398400 nfp_uma.c:43] Creating pool 'tcpcb', nitems=1024 size=784 total=802816
I 22 9:3431398400 nfp_uma.c:43] Creating pool 'tcptw', nitems=1024 size=80 total=81920
I 22 9:3431398400 nfp_uma.c:43] Creating pool 'syncache', nitems=240 size=168 total=40320
I 22 9:3431398400 nfp_uma.c:43] Creating pool 'tcpreass', nitems=320 size=48 total=15360
I 22 9:3431398400 nfp_uma.c:43] Creating pool 'sackhole', nitems=4096 size=40 total=163840
I 22 9:3431398400 nfp_uma.c:43] Creating pool 'icmp_inpcb', nitems=8 size=1160 total=9280
I 23 9:3431398400 nfp_ipsec.c:188] IPsec not supported with SP. Disabling IPsec.
pktio/dpdk.c:1697:dpdk_open():Invalid DPDK interface name: ens5f1
I 29 9:3431398400 nfp_ifnet.c:218] Device 'ens5f1' addr  80:61:5f:0c:d5:f5
I 29 9:3431398400 nfp_ifnet.c:241] Device 'ens5f1' MTU=1500
I 29 9:3431398400 nfp_init.c:655] Slow path threads on core 0
I 30 0:2736768000 nfp_cli_thread.c:54] CLI server started on core 0


ODP system info
---------------
ODP API version: 1.37.1
CPU model:       Intel(R) Xeon(R) CPU E5-2678 v3
CPU freq (hz):   2000000000
Cache line size: 64
Core count:      12

Running ODP appl: "nfp_webserver"
-----------------
IF-count:        1
Using IFs:       ens5f1

Num worker threads: 4
first CPU:          8
cpu mask:           0xF00
CLI: debug 0


CLI: loglevel set debug


CLI: ifconfig fp0 192.168.56.33/24

I 31 9:3431398400 nfp_rt_mtrie_lookup.c:285] nfp_rt_rule_add inserted new rule vrf 0 prefix 192.168.56.33/32
I 31 9:3431398400 nfp_rt_mtrie_lookup.c:285] nfp_rt_rule_add inserted new rule vrf 0 prefix 192.168.56.0/24

I 33 0:2694804480 httpd.c:447] HTTP thread started
I 133 0:2694804480 httpd.c:256] Using nfp_select
----

In this example, the packets received by ens5f1 are captured by ODP and
forwarded to the NFP application (fastpath).
If no fastpath operations are applicable for some packets, they are forwarded
to a 'sp0' TUN/TAP interface created by NFP (slowpath).

By default webserver will listen port 2048 so you can verify its functionality
e.g. with following command:

 curl -i -XGET http://192.168.56.33:2048/index.html

Use Ctrl+C in order to terminate the application.

Note that NFP provides bash scripts to manage linux interface isolation: see 
nfp_linux_interface_acquire.sh and nfp_linux_interface_release.sh from
'./scripts/' folder.

See ./example/README file for more details on available example applications.

=== Using NFP CLI

Telnet based Command line interface (CLI) can be used for configuring and
debugging NFP. Basic CLI commands provide following functions:

- setting debug level
- capturing traffic and dumping it to the console or to a PCAP file
- showing and configuring ARP table
- showing and configuring interfaces and tunnels
- showing and configuring routes
- showing and clearing statistics

CLI thread may be started at NFP initialization time or by directly calling API
nfp_cli_start_os_thread().

CLI can be accessed with `telnet 127.0.0.1 2345` (or `telnet 0 2345` for short)
command:

----
[nfp]# telnet 127.0.0.1 2345
Trying 127.0.0.1...
Connected to 127.0.0.1.
Escape character is '^]'.

--==--==--==--==--==--==--
-- WELCOME to NFP CLI --
--==--==--==--==--==--==--

 Type 'help' for information on available commands...

> help
Use 'help' command to display information on CLI commands:
  help <command>
    command: alias, address, arp, debug, exit, ifconfig, ipsec, loglevel, netstat, ping, ps, route, show, shutdown, stat, sysctl

Commands summary:
-----------------
alias    : configures command aliasses
address  : configures additional IPv4 addresses on an interface
arp      : configures ARP entries
debug    : configures packet capture and printing options
dns      : displays status of the dns component
exit     : closes the CLI connection
ifconfig : configures interfaces
ipsec    : configures IPsec SA and SP
loglevel : configures log level
netstat  : prints network connections
ping     : pings IPv4/IPv6 addresses
ps       : prints information on running threads/processes
route    : configures routes
show     : displays status of NFP components (e.g arp, root) 
shutdown : shutdown NFP
stat     : prints statistics 
sysctl   : configures system parameters

>
----

For example, current IP configuration can be shown with `ifconfig` command:

----
> ifconfig
fp0	(12) (enp1s0f0) slowpath fwd: on (sp0)
	Link encap:Ethernet	HWaddr:  80:61:5f:0c:d5:f4
	inet addr:192.168.100.10	Bcast:192.168.100.255	Mask:255.255.255.0
	inet6 addr: fe80:0000:0000:0000:8261:5fff:fe0c:d5f4 Scope:Link
	MTU: 1500
	RX: bytes:0 packets:0 dropped:0 errors:0
	TX: bytes:55256 packets:177 dropped:0 error:0

 ...
----

CLI commands can also be read from a file and executed during application
startup.

=== Starting applications with NFP netwrap

Some native Linux applications which use TCP/IP socket API can be run as such
on top of NFP. This requires that the applications include netwrap_proc and
netwrap_crt libraries into LD_PRELOAD list. netwrap_proc library
implements ODP/NFP configuration and startup of processing threads whereas
netwrap_crt implements symbol overloading and argument conversion for the
following native calls: socket(), close(), shutdown(), bind(), accept(),
accept4(), listen(), connect(), read(), write(), recv(), send(), getsockopt(),
setsockopt(), writev(), sendfile64(), select(), ioctl() and fork().

A script (./scripts/nfp_netwrap.sh) is provided in order to make utilization of
this feature more friendly.

Note that utilizing NFP netwrap has some restrictions:

 - application needs to run as superuser

 - slow path support is disabled on all interfaces

 - specific socket configuration needs to be activated. Use
./config/nfp_flv_netwrap_default.conf configuration file.

In the following example we start a native application on top of NFP:

 - Update load library path
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<nfp_path>/nfp/lib/x86/shared:<nfp_path>/nfp/lib/.libs

 - Set NFP initialization parameters

export NFP_NETWRAP_ENV="-i enp1s0f0@192.168.100.10/24"

 - Set NFP configuration

export NFP_CONF_FILE=<nfp_path>/nfp/config/nfp_flv_netwrap_default.conf

Now, the application can be started:

----
[nfp_ent]# ./scripts/nfp_netwrap.sh <app_path>/application
HW time counter freq: 2494224745 hz

System config:
  system.thread_count_max: 256

Pool config:
  pool.local_cache_size: 256
  pool.burst_size: 32
  pool.pkt.max_num: 262143
  pool.pkt.max_len: 65536
  pool.pkt.base_align: 64
  pool.buf.min_align: 64

Queue config:
  queue_basic.max_queue_size: 8192
  queue_basic.default_queue_size: 4096

Using scheduler 'basic'
Scheduler config:
  sched_basic.prio_spread: 4
  sched_basic.prio_spread_weight: 63
  sched_basic.load_balance: 1
  sched_basic.burst_size_default[] =  32  32  32  32  32  16   8   4
  sched_basic.burst_size_max[] =     255 255 255 255 255  16  16   8
  sched_basic.group_enable.all: 1
  sched_basic.group_enable.worker: 1
  sched_basic.group_enable.control: 1
  dynamic load balance: ON

Packet IO config:
  pktio.pktin_frame_offset: 0

PKTIO: initialized loop interface.
PKTIO: initialized dpdk pktio, use export ODP_PKTIO_DISABLE_DPDK=1 to disable.
PKTIO: initialized pcap interface.
PKTIO: initialized null interface.
PKTIO: initialized socket mmap, use export ODP_PKTIO_DISABLE_SOCKET_MMAP=1 to disable.
PKTIO: initialized socket mmsg, use export ODP_PKTIO_DISABLE_SOCKET_MMSG=1 to disable.
EAL: Detected 12 lcore(s)
EAL: Detected 1 NUMA nodes
EAL: Auto-detected process type: PRIMARY
EAL: Detected static linkage of DPDK
EAL: Multi-process socket /var/run/dpdk/130184_/mp_socket
EAL: Selected IOVA mode 'PA'
EAL: No available hugepages reported in hugepages-1048576kB
EAL: Probing VFIO support...
EAL: VFIO support initialized
EAL: Probe PCI driver: net_ixgbe (8086:10fb) device: 0000:03:00.0 (socket 0)
EAL: Probe PCI driver: net_ixgbe (8086:10fb) device: 0000:03:00.1 (socket 0)
EAL: No legacy callbacks, legacy socket not created

DPDK version: DPDK 20.11.4-rc1
I 69 3:1810664448 nfp_uma.c:43] Creating pool 'udp_inpcb', nitems=1024 size=1160 total=1187840
I 70 3:1810664448 nfp_uma.c:43] Creating pool 'udp_inphd', nitems=1024 size=40 total=40960
I 70 3:1810664448 nfp_uma.c:43] Creating pool 'tcp_inpcb', nitems=1024 size=1160 total=1187840
I 70 3:1810664448 nfp_uma.c:43] Creating pool 'tcp_inphd', nitems=1024 size=40 total=40960
I 70 3:1810664448 nfp_uma.c:43] Creating pool 'tcpcb', nitems=1024 size=784 total=802816
I 70 3:1810664448 nfp_uma.c:43] Creating pool 'tcptw', nitems=1024 size=80 total=81920
I 71 3:1810664448 nfp_uma.c:43] Creating pool 'syncache', nitems=240 size=168 total=40320
I 71 3:1810664448 nfp_uma.c:43] Creating pool 'tcpreass', nitems=320 size=48 total=15360
I 71 3:1810664448 nfp_uma.c:43] Creating pool 'sackhole', nitems=4096 size=40 total=163840
I 71 3:1810664448 nfp_uma.c:43] Creating pool 'icmp_inpcb', nitems=8 size=1160 total=9280
I 36 1:3353287680 nfp_ifnet.c:219] Device 'enp1s0f0' addr  80:61:5f:0c:d5:f4
I 36 1:3353287680 nfp_ifnet.c:241] Device 'enp1s0f0' MTU=1500

ODP system info
---------------
ODP API version: 1.37.1
CPU model:       Intel(R) Xeon(R) CPU E5-2678 v3
CPU freq (hz):   1200000000
Cache line size: 64
Core count:      12

Running ODP appl: "nfp_netwrap"
-----------------
IF-count:        1
Using IFs:       enp1s0f0

Num worker threads: 10
first CPU:          2
cpu mask:           0xFFC
I 90 3:1810664448 nfp_rt_mtrie_lookup.c:285] nfp_rt_rule_add inserted new rule vrf 0 prefix 192.168.100.10/32
I 90 3:1810664448 nfp_rt_mtrie_lookup.c:285] nfp_rt_rule_add inserted new rule vrf 0 prefix 192.168.100.0/24
I 92 0:1002431488 nfp_cli_thread.c:54] CLI server started on core 0

I 190 3:1810664448 app_main.c:206] End Netwrap processing constructor()

----

== Overview of NFP API

NFP public API header files can be found in ./include/api/ folder at the
<<GitHub>> project repository.

NFP provides APIs for:

- initialisation and termination of NFP (nfp_init.h)
- creating, configuration and deleting interfaces (nfp_ifnet_portconf.h)
- handling routing and ARP tables (nfp_route_arp.h)
- packet Ingress and Egress processing (nfp_pkt_processing.h)
- hooks for IP local, IP forwarding and GRE (nfp_hook.h)
- NFP socket API (nfp_socket.h)
- timer callbacks (nfp_timer.h)
- packet and performance statistics (nfp_stat.h)
- debugging and packet capture (nfp_debug.h)
- logging (nfp_log.h)
- customizing CLI commands (nfp_api_cli.h)
- handling Management Information Base entries (nfp_sysctl.h)

In addition, API folder contains a number of protocol specific header files
containing data structures, macros and constants for accessing and manipulating
packet headers and data.

At UDP and TCP level, NFP uses an optimized callback based zero-copy socket API
which enables the usage of the complete packet, including metadata, in user
space. This is done without copy operations typically used by the traditional
BSD sockets. Termination of protocols with BSD socket interface for legacy
applications is also supported.

== Designing NFP applications

=== Application structure

In an NFP application, one instance of NFP runs across all the assigned data
plane cores. 

On the cores allocated to fastpath processing, ODP starts only one thread or
process (workers) where the event/packet dispatcher, NFP and the user
application code runs.

The dispatcher functions can be configure per core.

If legacy BSD socket APIs are used, they need to run on a separate core or cores
(on control threads) in order to not interfere with the NFP fastpath processing.

==== Packet input modes

Incoming packets can be received by an NFP application either directly or via
scheduled receive queues.

===== Direct mode

Direct mode is designed to support poll-based packet processing. In direct mode,
received packets are stored by ODP into one or more packet IO queues and can be
retrieved by worker threads with odp_pktin_recv() call. Note that applications
cannot perform enqueues to these queues. Packets can be transmitted to the
packet IO by calling odp_pktout_send().

Optional RSS hashing functionality can be enabled for distributing packets to
different input queues.

===== Scheduled mode

Scheduled mode integrates RX packet processing with the ODP event model. In case
of scheduled mode incoming packets are distributed by ODP scheduler to multiple
scheduled queues which have associated scheduling attributes like priority,
scheduler group and synchronization mode (parallel, atomic, ordered).
Information about scheduled packets is then provided to requesting threads as
events.

Worker threads of an NFP application can then use either default or their own
event dispatchers for consuming incoming events with odp_schedule() or
odp_schedule_multi() function call and processing them further.

See ODP <<Users-Guide>> for more details about packet input/output modes.

=== Initialization and startup

==== Initializing ODP

ODP Initialization can be done either explicit by the application or
implicit by NFP initialization rutine.

On explicit initialization, the first ODP API that must be called by an ODP/NFP 
application is odp_init_global(). Calling odp_init_global() establishes the 
ODP API framework and should be called only once per application. Following the
global initialization, each thread in turn calls odp_init_local(). This
establishes the local ODP thread context for that thread.
The created ODP instance is then passed as argument to nfp_initialize().

The implict initialization takes place when NFP_ODP_INSTANCE_INVALID value is
passed as argument to nfp_initialize(). NFP will creat and manage the ODP
instance.

==== Initializing NFP

The first NFP API that must be called by an ODP/NFP application is
nfp_initialize_param(). It initializes the NFP initialization
parameter to the default values. The structure contains such global
parameters as interface count, interface names, packet processing hooks, packet
input mode etc. These parameters can, if necessary, be updated by the
application before passing them to the next function to be called,
nfp_initialize().

Shutdown is the logical reverse of the initialization procedure when
nfp_terminate() function is called by respective threads in order to free
ODP/NFP resources properly.

==== Assigning processor cores

NFP application is responsible for mapping processor cores to its worker
threads. Number of available cores can be checked with odp_cpu_count() call. By
default core 0 (and core 1 in some ODP versions) is used for operating system
background tasks (this value is a part of NFP initialization parameter structure)
so it is recommended to start mapping from core 1 (or 2). odp_cpumask_* functions
of ODP API can be used for initialization of the CPU mask. Configured CPU mask
can be later given as a parameter to nfp_thread_create() or nfp_process_fork_n()
functions which will create and start worker threads or processes on the
assigned cores.

See <<ODP_API>> for more information about ODP API and helper functions.

Alternatively, nfp_get_default_worker_cpumask() may be used to get the
default worker distribution CPU mask, starting from the number of requested
workers and number of cores available.

==== Allocating packet IO interfaces

nfp_initialize() function creates respective packet IO instances for all the
interfaces included into NFP initialization parameter structure. Some of the
properties, such as packet input and output modes, of the packet IO instances
can be configured through the global initialization parameters passed to
nfp_initialize().

If an NFP application needs packet IO configuration that is not possible through
nfp_initialize() (e.g. enabling multiple input or output queues per interface),
it must create respective packet IO instances after NFP initialization through
the nfp_ifport_net_create() API. This will require the following steps:

- initialize the default network interface parameters by calling
  nfp_ifport_net_param_init() routine

- set non-default values for the parameters

- call nfp_ifport_net_create() function for each interface

For example, the following code sample will set the
non-default parameters and create packet IO objects:

----
/** create_interfaces_direct_rss() Create NFP interfaces with
  * pktios open in direct mode, thread unsafe and using RSS with
  * hashing by IPv4 addresses and TCP ports
  *
  * @param if_count int  Interface count
  * @param if_names char** Interface names
  * @param tx_queue int Number of requested transmission queues
  *    per interface
  * @param rx_queue int Number of requested receiver queues per
  *    interface
  * @return int 0 on success, -1 on error
  *
  */
 static int create_interfaces_direct_rss(int if_count, char **if_names,
         int tx_queues, int rx_queues)
 {
         nfp_ifport_net_param_t if_param;
         odp_pktio_param_t pktio_param;
         odp_pktin_queue_param_t pktin_param;
         odp_pktout_queue_param_t pktout_param;
         int i;

         nfp_ifport_net_param_init(&if_param);
         if_param.pktio_param = &pktio_param;
         if_param.pktin_param = &pktin_param;
         if_param.pktout_param = &pktout_param;
         if_param.if_sp_mgmt = 1;

         odp_pktio_param_init(&pktio_param);
         pktio_param.in_mode = ODP_PKTIN_MODE_DIRECT;
         pktio_param.out_mode = ODP_PKTOUT_MODE_DIRECT;

         odp_pktin_queue_param_init(&pktin_param);
         pktin_param.op_mode = ODP_PKTIO_OP_MT_UNSAFE;
         pktin_param.hash_enable = 1;
         pktin_param.hash_proto.proto.ipv4_tcp = 1;
         pktin_param.num_queues = rx_queues;

         odp_pktout_queue_param_init(&pktout_param);
         Pktout_param.op_mode = ODP_PKTIO_OP_MT_UNSAFE;
         pktout_param.num_queues = tx_queues;

         for (i = 0; i < if_count; i++)
                 if (nfp_ifport_net_create(if_names[i],
                                 &if_param,
                                 NULL, NULL) < 0) {
                         NFP_ERR("Failed to init interface %s",
                                 if_names[i]);
                         return -1;
                 }

         return 0;
 }
----

==== RSS and multiqueue support

Many NICs provide multiple transmit and receive queues, allowing packets
received by the NIC to be assigned to one of its receive queues. Maximum number
of input/output queues available in used NICs can be checked with ODP function
odp_pktio_capability(). Desired number of input/output queues can then be
provided as part of odp_pktin_queue_param_t and odp_pktout_queue_param_t
parameter structures to nfp_ifport_net_create() function (see the example in the
previous chapter).

More than one input queue require input hashing or classifier setup. In the
previous example input hashing is enabled and hash type is set to ipv4_tcp
meaning that used NIC should compute hash values over the following header
fields:

- source IPv4 address

- destination IPv4 address

- source TCP Port

- destination TCP Port

As a consequence, packets coming from the same TCP flow will be directed to the
same input queue.

==== Starting worker and control threads

NFP application can use nfp_thread_create() for creating worker and control
threads/processes. The function takes four input arguments: thread table,
number of threads to start, CPU mask and NFP thread parameter. CPU mask is used
for setting CPU affinity for the created threads/processes and can be initialized
e.g. with nfp_get_default_worker_cpumask() call. NFP thread parameter should be
populated with following thread specific parameters:

- thread entry point function (e.g. event dispatcher)

- optional argument for the thread entry point function

- ODP thread type (ODP_THREAD_WORKER or ODP_THREAD_CONTROL)

Created threads can be joined with nfp_thread_join() function.

==== Default/user event dispatcher

NFP library provides a default event dispatcher function (int 
default_event_dispatcher(void *arg)) which can be run by worker threads of an
NFP applications on each dedicated processor core. This function provides basic
event handling functionality for packet receiving, timer expiration, buffer
freeing and crypto API completion events. It can be provided as a parameter when
creating worker threads/processes with nfp_thread_create() function.
Default event dispatcher function takes one parameter which is a function to be
used for processing incoming packets (e.g. nfp_eth_vlan_processing() implemented
by NFP library).
NFP 6.0 adds a multi-packet API aware default event dispatcher (int
default_event_dispatcher_multi(void *arg)) that takes as parameter a multi
packet processing function (e.g. nfp_eth_vlan_processing_multi()).

An NFP application can also implement its own event dispatchers for worker
threads. Custom event dispatchers can use e.g. odp_pktin_recv() (in case
of direct mode) and odp_schedule()/odp_schedule_multi() (in case of scheduled
mode) functions so as other ODP/NFP features for retrieving and handling
incoming packets and events.

==== Starting CLI thread

When dispatcher threads are running, further application logic can be
launched. In order to enable NFP CLI, a dedicated CLI thread should be started on
the management core (not competing for CPU cycles with the worker threads) by
calling nfp_cli_start_os_thread() function. In addition to the CLI threads,
the nfp_cli_process_file() function can be used to process CLI commands.
It takes as argument a text file (named in examples as nfp.cli) that contains
NFP CLI commands which will be executed in the context of the caller's thread.

Below is an example of NFP CLI file (from example/webserver/nfp.cli):

 debug 0
 loglevel set debug
 ifconfig fp0 192.168.56.33/24

nfp_cli_add_command() function can be used by an NFP application in order to add
customized CLI commands. nfp_cli_stop_os_thread() function is used for
termination of the CLI thread.

=== Synchronization issues

ODP scheduler provides event synchronization services that simplify application
programming in a parallel processing environment.

ODP synchronization mode determines how the scheduler handles processing of
multiple events originating from the same queue.

In ODP_SCHED_SYNC_NONE mode different events from parallel queues can be
scheduled simultaneously to different worker threads. In this case application
is responsible for possibly needed synchronization during event handling.

In ODP_SCHED_SYNC_ATOMIC mode only a single worker thread receives events from a
given queue at a time. Events scheduled from atomic queues thus can be processed
lock free because the locking is being done implicitly by the scheduler.

In ODP_SCHED_SYNC_ORDERED mode the scheduler dispatches multiple events from the
queue in parallel to different threads, however the scheduler also ensures that
the relative sequence of these events on output queues is identical to their
sequence from their originating ordered queue.

See ODP <<Users-Guide>> for more details about queue synchronization modes.

=== Packet processing

The packet processing is handled in NFP through a series of self-contained
processing functions which means that traffic can be inserted at various places
in the packet processing flow.

NFP applications can use packet processing functions from nfp_pkt_processing.h
API for handling packets received by worker threads from Ethernet interfaces and
Linux kernel. The packet processing component also provides API for sending
packets.

See NFP technical <<overview>> for ingress/egress packet processing diagrams.

Overall packet processing performance can be further improved by taking into use
available hardware acceleration functions for packet validation, checksum
calculation, cryptographic transformations as well as optimized memory/buffers
operations. Such HW acceleration capabilities are platform specific and can be
configured, if available, with respective ODP API.

=== Timers

NFP applications can uses functions from nfp_timer.h API in order to
start/cancel ODP timers so as handle ODP timer events. Applications can also
register timeout callback functions that will be posted on the specified CPU
timer queue.

=== Hooks

NFP applications can implement and register its own functions for processing
specific received packets or specific packets to be sent by NFP. Below is the
list of available hook handles from include/api/nfp_hook.h:

 enum nfp_hook_id {
         NFP_HOOK_LOCAL = 0,     /**< Registers a function to handle all packets
                                         with processing at IP level */
         NFP_HOOK_LOCAL_IPv4,    /**< Registers a function to handle all packets
                                         with processing at IPv4 level */
         NFP_HOOK_LOCAL_IPv6,    /**< Registers a function to handle all packets
                                         with processing at IPv6 level */
         NFP_HOOK_LOCAL_UDPv4,   /**< Registers a function to handle all packets
                                         with processing at UDP IPv4 level */
         NFP_HOOK_LOCAL_UDPv6,   /**< Registers a function to handle all packets
                                         with processing at UDP IPv6 level */
         NFP_HOOK_FWD_IPv4,      /**< Registers a function to handle all IPv4
                                         packets that require forwarding */
         NFP_HOOK_FWD_IPv6,      /**< Registers a function to handle all IPv6
                                         packets that require forwarding */
         NFP_HOOK_GRE,           /**< Registers a function to handle GRE tunnels
                                         not registered to NFP */
         NFP_HOOK_OUT_IPv4,      /**< Registers a function to handle all IPv4
                                         packets to be sent by NFP*/
         NFP_HOOK_OUT_IPv6,      /**< Registers a function to handle all IPv6
                                         packets to be sent by NFP*/
         NFP_HOOK_MAX
 };

Hook registration is done during application startup by providing pkt_hook table
to nfp_initialize() function. Some example applications (e.g. fpm) contain an
example of hook registration.

== Using NFP socket interface

At UDP and TCP level, NFP library implements an optimized zero-copy socket API
which enables usage of the complete packet, including metadata, in user space.
NFP applications can implement and register its own callback functions for
reading on sockets and handling TCP accept events.

Also legacy BSD socket interface is supported.

NFP socket API is described in include/api/nfp_socket.h.

Some NFP example applications (e.g. udpecho, webserver2, tcpperf) contain
examples of NFP socket API usage.

== Using NFP with ODP-DPDK

DPDK is supported by NFP through the ODP-DPDK implementation of ODP. NFP
repository contains a script (scripts/devbuild_all.sh) for building NFP
on top of ODP-DPDK.

The script will download and build compatible versions of DPDK, ODP and NFP.

Before launching NFP applications the following things should be
checked/adjusted in DPDK-based setups:

- Check current hugepage settings:

 cat /proc/meminfo | grep HugePages

- Adjust total number of hugepages if needed:

 sysctl -w vm.nr_hugepages=2048

- Insert the preferred kernel module (if needed):

 For igb_uio:
 /sbin/modprobe uio
 ulimit -Sn 2048
 insmod <dpdk path>/kmod/igb_uio.ko

 For uio_pci_generic:
 modprobe uio_pci_generic

- Check the current status of network interfaces:

 dpdk/usertools/dpdk-devbind.py -s

- Unbind desired interface(s) from using any other driver and bind it to
the preferred driver (e.g. igb_uio or uio_pci_generic):

 ifconfig <IF name> down
 dpdk/usertools/dpdk-devbind.py -u <domain:bus:slot.func>
 dpdk/usertools/dpdk-devbind.py --bind=uio_pci_generic <domain:bus:slot.func>

Note that you cannot use original names (e.g. eth0, eth1 etc.) for the
interfaces controlled by DPDK; those interfaces can be referenced as '0', '1'
etc. instead.

Now, the application can be started:

  ./example/fpm/nfp_fpm -i 0,1 -c 4 -f ./nfp.cli

Check <<DPDK>> documentation for more DPDK related information.

== Using NFP in virtualized environments and containers

In addition to baremetal environment, NFP applications can be run in virtual
machines and docker containers. Some things should be taken into account in such
cases.

1. Multiqueuing is disabled by default in virtio interfaces. For example, in
OpenStack based clouds the following thing should be done in order to enable
multiqueuing in virtual machines:

- hw_vif_multiqueue_enabled property should be set to "yes" when creating glance
images

- hw:vif_number_queues property should be set to desired value for used nova
flavors

- inside virtual machines combined number of tx/rx queues should be set with the
following command:

 ethtool -L <interface name> combined <number of queues>

2. Some additional parameters should be provided when starting containers with
NFP applications in order to enable needed memory and network features:

 docker run -it --ulimit memlock=8192000000:8192000000 --cap-add=NET_ADMIN --device=/dev/net/tun nfp

After starting a container needed networks can be created and connected to it,
e.g.:

 docker network create --driver bridge nfp_net
 docker network connect nfp_net <container ID>

Note that offloading of generic IP rx/tx checksum calculation is usually
enabled by default for both physical and virtual network interfaces. This may
result in a situation when TCP packets sent from one container or virtual
machine to another (inside the same physical server) will not contain valid
checksum and NFP will drop them. One possible workaround to this is to disable
tx checksumming for the sending interface/bridge with `ethtool -K <interface
name> tx off` command.

== Tools for performance measurements

A wide variety of HW/SW tools exist for measuring performance of different
layers of network stacks. Below are just few examples of free SW tools suitable
for benchmarking NFP applications.

1. <<wrk>> HTTP benchmarking tool can be used with webserver like applications.

 git clone https://github.com/wg/wrk.git
 cd wrk
 make
 ./wrk --threads 4 --connections 8 --duration 10s --timeout 1 --latency http://11.0.0.22:2048/index.html

2. tcpperf is a iperf-like NFP test application which can be used for TCP
benchmarking, see `tcpperf --help` for more details.

== Troubleshooting hints

=== Packet monitoring

Incoming/outgoing packets can be monitored using `debug` command of CLI. For
example, in order to print all the packets into a text file (packets.txt), give
following command:

 debug 0xf

An example of the output:

----
# cat /root/nfp/packets.txt

 *************
 [2] NIC to FP: 379.445
  08:00:27:78:c5:75 -> 08:00:27:24:a9:26
   IP len=60 TCP 10.10.10.101:52263 -> 10.10.10.102:2048
    seq=0xdd899b05 ack=0x0 off=10
    flags=S win=29200 sum=0x40 urp=0

 *************
 [2] FP to NIC: 379.446
  08:00:27:24:a9:26 -> 08:00:27:78:c5:75
   IP len=60 TCP 10.10.10.102:2048 -> 10.10.10.101:52263
    seq=0x6e2ff56f ack=0xdd899b06 off=10
    flags=SA win=65535 sum=0xe660 urp=0

 *************
 [2] NIC to FP: 379.446
  08:00:27:78:c5:75 -> 08:00:27:24:a9:26
   IP len=52 TCP 10.10.10.101:52263 -> 10.10.10.102:2048
    seq=0xdd899b06 ack=0x6e2ff570 off=8
    flags=A win=229 sum=0x1445 urp=0
----

Check `debug help` output for more details.

=== Debug logs

By default, only "info", "warning" and "error" log messages can be displayed.
In order to enable the "debug" level logs, a version of NFP library compiled
with --enable-debug option is needed.

Log level can be selected with API or from CLI:
e.g. in nfp.cli file:

 loglevel set debug

== Known restrictions

Socket based packet IO doesn't currently support multiqueuing which means that
only one input/output queue can be used in DIRECT_RSS mode with linux-generic
implementation of ODP. There is no such restriction when using DPDK or netmap
based packet IO.

RSS hashing is not currently supported by virtio interfaces. As a result, it is
not possible to ensure that e.g. packets from the same TCP flow will be always
received by the same worker thread/process.

== References

* [[[NetinoFastPath]]] NetinoFastPath project homepage
  http://netinosoft.org

* [[[GitHub]]] NetinoFastPath in GitHub
  https://github.com/NetInoSoftware/nfp/

* [[[OpenFastPath]]] OpenFastPath project homepage
  http://www.openfastpath.org/

* [[[OpenDataPlane]]] OpenDataPlane project homepage
  https://www.opendataplane.org/

* [[[overview]]] OpenFastPath technical overview
  http://www.openfastpath.org/index.php/service/technicaloverview/

* [[[Users-Guide]]] ODP Users-Guide
  https://docs.opendataplane.org/snapshots/odp-publish/generic/usr_html/master/latest/linux-generic/output/users-guide.html

* [[[ODP_API]]] OpenDataPlane API documentation
  https://www.opendataplane.org/api-documentation/

* [[[DPDK]]] DPDK documentation
  http://dpdk.org/doc/guides/index.html

* [[[wrk]]] HTTP benchmarking tool
  https://github.com/wg/wrk

* [[[libconfig]]] libconfig – C/C++ Configuration File Library
  http://www.hyperrealm.com/libconfig/libconfig.html
